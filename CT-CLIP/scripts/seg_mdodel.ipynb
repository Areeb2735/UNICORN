{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SingleDeconv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super().__init__()\n",
    "        self.block = nn.ConvTranspose3d(in_planes, out_planes, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class SingleConv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=((kernel_size - 1) // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Conv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SingleConv3DBlock(in_planes, out_planes, kernel_size),\n",
    "            nn.BatchNorm3d(out_planes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Deconv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SingleDeconv3DBlock(in_planes, out_planes),\n",
    "            SingleConv3DBlock(out_planes, out_planes, kernel_size),\n",
    "            nn.BatchNorm3d(out_planes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Conv3DBlock_init(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "class UNETR(nn.Module):\n",
    "    def __init__(self, img_shape=(128, 128, 128), input_dim=4, output_dim=3, embed_dim=768, patch_size=16, num_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.img_shape = img_shape\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = 12\n",
    "        self.ext_layers = [3, 6, 9, 12]\n",
    "\n",
    "        self.patch_dim = [int(x / patch_size) for x in img_shape]\n",
    "\n",
    "        # U-Net Decoder\n",
    "        self.decoder0 = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(input_dim, 32, 3),\n",
    "                Conv3DBlock(32, 64, 3)\n",
    "            )\n",
    "\n",
    "        self.decoder3 = \\\n",
    "            nn.Sequential(\n",
    "                Deconv3DBlock(embed_dim, 512),\n",
    "                Deconv3DBlock(512, 256),\n",
    "                Deconv3DBlock(256, 128)\n",
    "            )\n",
    "\n",
    "        self.decoder6 = \\\n",
    "            nn.Sequential(\n",
    "                Deconv3DBlock(embed_dim, 512),\n",
    "                Deconv3DBlock(512, 256),\n",
    "            )\n",
    "\n",
    "        self.decoder9 = \\\n",
    "            Deconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder12_upsampler = \\\n",
    "            SingleDeconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder9_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(1024, 512),\n",
    "                Conv3DBlock(512, 512),\n",
    "                Conv3DBlock(512, 512),\n",
    "                SingleDeconv3DBlock(512, 256)\n",
    "            )\n",
    "\n",
    "        self.decoder6_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(512, 256),\n",
    "                Conv3DBlock(256, 256),\n",
    "                SingleDeconv3DBlock(256, 128)\n",
    "            )\n",
    "\n",
    "        self.decoder3_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(256, 128),\n",
    "                Conv3DBlock(128, 128),\n",
    "                SingleDeconv3DBlock(128, 64)\n",
    "            )\n",
    "\n",
    "        self.decoder0_header = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(128, 64),\n",
    "                Conv3DBlock(64, 64),\n",
    "                SingleConv3DBlock(64, output_dim, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z0, z3, z6, z9, z12 = x, *z\n",
    "        # z3 = z3.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        # z6 = z6.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        # z9 = z9.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        # z12 = z12.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "\n",
    "        z12 = self.decoder12_upsampler(z12)\n",
    "        z9 = self.decoder9(z9)\n",
    "        z9 = self.decoder9_upsampler(torch.cat([z9, z12], dim=1))\n",
    "        z6 = self.decoder6(z6)\n",
    "        z6 = self.decoder6_upsampler(torch.cat([z6, z9], dim=1))\n",
    "        z3 = self.decoder3(z3)\n",
    "        z3 = self.decoder3_upsampler(torch.cat([z3, z6], dim=1))\n",
    "        z0 = self.decoder0(z0)\n",
    "        output = self.decoder0_header(torch.cat([z0, z3], dim=1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 48, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "decoder12_upsampler = \\\n",
    "    SingleDeconv3DBlock(512, 512)\n",
    "\n",
    "z12 = decoder12_upsampler(torch.randn(1, 512, 24, 24, 24))\n",
    "print(z12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 48, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "decoder9 = \\\n",
    "    Deconv3DBlock(512, 512)\n",
    "\n",
    "z9 = decoder9(torch.randn(1, 512, 24, 24, 24))\n",
    "print(z9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "decoder9_upsampler = \\\n",
    "    nn.Sequential(\n",
    "        Conv3DBlock(1024, 512),\n",
    "        Conv3DBlock(512, 512),\n",
    "        Conv3DBlock(512, 512),\n",
    "        SingleDeconv3DBlock(512, 256)\n",
    "    )\n",
    "\n",
    "z9 = decoder9_upsampler(torch.cat([z9, z12], dim=1))\n",
    "print(z9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "decoder6 = \\\n",
    "    nn.Sequential(\n",
    "        Deconv3DBlock(512, 512),\n",
    "        Deconv3DBlock(512, 256),\n",
    "    )\n",
    "\n",
    "z6 = decoder6(torch.randn(1, 512, 24, 24, 24))\n",
    "print(z6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 192, 192, 192])\n"
     ]
    }
   ],
   "source": [
    "decoder6_upsampler = \\\n",
    "    nn.Sequential(\n",
    "        Conv3DBlock(512, 256),\n",
    "        Conv3DBlock(256, 256),\n",
    "        SingleDeconv3DBlock(256, 128)\n",
    "    )\n",
    "\n",
    "z6 = decoder6_upsampler(torch.cat([z6, z9], dim=1))\n",
    "print(z6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 192, 192, 192])\n"
     ]
    }
   ],
   "source": [
    "decoder3 = \\\n",
    "    nn.Sequential(\n",
    "        Deconv3DBlock(512, 512),\n",
    "        Deconv3DBlock(512, 256),\n",
    "        Deconv3DBlock(256, 128)\n",
    "    )\n",
    "\n",
    "z3 = decoder3(torch.randn(1, 512, 24, 24, 24))\n",
    "print(z3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 384, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "decoder0 = nn.Sequential(\n",
    "    Conv3DBlock_init(1, 32, 3),  \n",
    "    nn.Upsample(size=(384, 384, 384), mode='trilinear', align_corners=False),\n",
    "    Conv3DBlock_init(32, 64, 3)\n",
    ")\n",
    "\n",
    "z0 = decoder0(torch.randn(1, 1, 240, 480, 480))\n",
    "print(z0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 384, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "decoder0_header = \\\n",
    "    nn.Sequential(\n",
    "        Conv3DBlock(128, 64),\n",
    "        Conv3DBlock(64, 64),\n",
    "        SingleConv3DBlock(64, 1, 1)\n",
    "    )\n",
    "\n",
    "output = decoder0_header(torch.cat([z0, z0], dim=1))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SingleDeconv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super().__init__()\n",
    "        self.block = nn.ConvTranspose3d(in_planes, out_planes, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class SingleConv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=((kernel_size - 1) // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Conv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SingleConv3DBlock(in_planes, out_planes, kernel_size),\n",
    "            nn.BatchNorm3d(out_planes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Deconv3DBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SingleDeconv3DBlock(in_planes, out_planes),\n",
    "            SingleConv3DBlock(out_planes, out_planes, kernel_size),\n",
    "            nn.BatchNorm3d(out_planes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Conv3DBlock_init(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "class UNETR(nn.Module):\n",
    "    def __init__(self, input_dim=1, output_dim=1, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.decoder0 = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock_init(input_dim, 32, 3),  \n",
    "                nn.Upsample(size=(96, 96, 96), mode='trilinear', align_corners=False),\n",
    "                Conv3DBlock_init(32, 64, 3),\n",
    "                Conv3DBlock(64, 256, 3)\n",
    "            )\n",
    "\n",
    "        self.decoder3 = \\\n",
    "            nn.Sequential(\n",
    "                Deconv3DBlock(embed_dim, 512),\n",
    "                Deconv3DBlock(512, 256),\n",
    "                Deconv3DBlock(256, 128)\n",
    "            )\n",
    "\n",
    "        self.decoder6 = \\\n",
    "            nn.Sequential(\n",
    "                Deconv3DBlock(embed_dim, 512),\n",
    "                Deconv3DBlock(512, 256),\n",
    "            )\n",
    "\n",
    "        self.decoder9 = \\\n",
    "            Deconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder12_upsampler = \\\n",
    "            SingleDeconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder9_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(1024, 512),\n",
    "                Conv3DBlock(512, 512),\n",
    "                Conv3DBlock(512, 512),\n",
    "                SingleDeconv3DBlock(512, 256)\n",
    "            )\n",
    "\n",
    "        self.decoder6_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(512, 256),\n",
    "                Conv3DBlock(256, 256),\n",
    "                SingleDeconv3DBlock(256, 128)\n",
    "            )\n",
    "\n",
    "        self.decoder3_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(256, 128),\n",
    "                Conv3DBlock(128, 128),\n",
    "                SingleDeconv3DBlock(128, 64)\n",
    "            )\n",
    "\n",
    "        self.decoder0_header = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(512, 64),\n",
    "                # Conv3DBlock(256, 64),\n",
    "                SingleConv3DBlock(64, output_dim, 1)\n",
    "            )\n",
    "    \n",
    "    def proj_feat(self, x, hidden_size, feat_size):\n",
    "        new_view = (x.size(0), *feat_size, hidden_size)\n",
    "        x = x.view(new_view)\n",
    "        new_axes = (0, len(x.shape) - 1) + tuple(d + 1 for d in range(len(feat_size)))\n",
    "        x = x.permute(new_axes).contiguous()\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, hidden_state):\n",
    "        z0, z3, z6, z9, z12 = image, *hidden_state\n",
    "\n",
    "        # z3 = self.proj_feat(z3, 512, (24, 24, 24))\n",
    "        # z6 = self.proj_feat(z6, 512, (24, 24, 24))\n",
    "        z9 = self.proj_feat(z9, 512, (24, 24, 24))\n",
    "        z12 = self.proj_feat(z12, 512, (24, 24, 24))\n",
    "\n",
    "        z12 = self.decoder12_upsampler(z12)\n",
    "        z9 = self.decoder9(z9)\n",
    "        z9 = self.decoder9_upsampler(torch.cat([z9, z12], dim=1))\n",
    "        # z6 = self.decoder6(z6)\n",
    "        # z6 = self.decoder6_upsampler(torch.cat([z6, z9], dim=1))\n",
    "        # z3 = self.decoder3(z3)\n",
    "        # z3 = self.decoder3_upsampler(torch.cat([z3, z6], dim=1))\n",
    "        z0 = self.decoder0(z0)\n",
    "        print(z0.shape, z9.shape)\n",
    "        output = self.decoder0_header(torch.cat([z0, z9], dim=1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNETR(\n",
       "  (decoder0): Sequential(\n",
       "    (0): Conv3DBlock_init(\n",
       "      (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Upsample(size=(96, 96, 96), mode='trilinear')\n",
       "    (2): Conv3DBlock_init(\n",
       "      (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(64, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder3): Sequential(\n",
       "    (0): Deconv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleDeconv3DBlock(\n",
       "          (block): ConvTranspose3d(512, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (1): SingleConv3DBlock(\n",
       "          (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Deconv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleDeconv3DBlock(\n",
       "          (block): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (1): SingleConv3DBlock(\n",
       "          (block): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Deconv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleDeconv3DBlock(\n",
       "          (block): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (1): SingleConv3DBlock(\n",
       "          (block): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder6): Sequential(\n",
       "    (0): Deconv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleDeconv3DBlock(\n",
       "          (block): ConvTranspose3d(512, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (1): SingleConv3DBlock(\n",
       "          (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Deconv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleDeconv3DBlock(\n",
       "          (block): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (1): SingleConv3DBlock(\n",
       "          (block): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder9): Deconv3DBlock(\n",
       "    (block): Sequential(\n",
       "      (0): SingleDeconv3DBlock(\n",
       "        (block): ConvTranspose3d(512, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      )\n",
       "      (1): SingleConv3DBlock(\n",
       "        (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "      (2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder12_upsampler): SingleDeconv3DBlock(\n",
       "    (block): ConvTranspose3d(512, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "  )\n",
       "  (decoder9_upsampler): Sequential(\n",
       "    (0): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): SingleDeconv3DBlock(\n",
       "      (block): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "  )\n",
       "  (decoder6_upsampler): Sequential(\n",
       "    (0): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): SingleDeconv3DBlock(\n",
       "      (block): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "  )\n",
       "  (decoder3_upsampler): Sequential(\n",
       "    (0): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): SingleDeconv3DBlock(\n",
       "      (block): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "  )\n",
       "  (decoder0_header): Sequential(\n",
       "    (0): Conv3DBlock(\n",
       "      (block): Sequential(\n",
       "        (0): SingleConv3DBlock(\n",
       "          (block): Conv3d(512, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): SingleConv3DBlock(\n",
       "      (block): Conv3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNETR()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 201.12 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 391.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m240\u001b[39;49m, \u001b[39m480\u001b[39;49m, \u001b[39m480\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device), [torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m13824\u001b[39;49m, \u001b[39m512\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device), torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m13824\u001b[39;49m, \u001b[39m512\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device), torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m13824\u001b[39;49m, \u001b[39m512\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device), torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m13824\u001b[39;49m, \u001b[39m512\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)])\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 142\u001b[0m, in \u001b[0;36mUNETR.forward\u001b[0;34m(self, image, hidden_state)\u001b[0m\n\u001b[1;32m    140\u001b[0m z12 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder12_upsampler(z12)\n\u001b[1;32m    141\u001b[0m z9 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder9(z9)\n\u001b[0;32m--> 142\u001b[0m z9 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder9_upsampler(torch\u001b[39m.\u001b[39;49mcat([z9, z12], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    143\u001b[0m \u001b[39m# z6 = self.decoder6(z6)\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m# z6 = self.decoder6_upsampler(torch.cat([z6, z9], dim=1))\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m# z3 = self.decoder3(z3)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39m# z3 = self.decoder3_upsampler(torch.cat([z3, z6], dim=1))\u001b[39;00m\n\u001b[1;32m    147\u001b[0m z0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder0(z0)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m, in \u001b[0;36mConv3DBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock(x)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mSingleConv3DBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock(x)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/torch/nn/modules/conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[1;32m    710\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    711\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    719\u001b[0m     )\n\u001b[0;32m--> 720\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[1;32m    721\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    722\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 201.12 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 391.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model(torch.randn(1, 1, 240, 480, 480).to(device), [torch.randn(1, 13824, 512).to(device), torch.randn(1, 13824, 512).to(device), torch.randn(1, 13824, 512).to(device), torch.randn(1, 13824, 512).to(device)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "mask = nib.load(str('/share/sda/mohammadqazi/project/hector/dataset/processed_samples_all/CHUM-001_mask_roi.nii.gz')).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((480, 480, 240), np.float64(0.0), np.float64(2.0))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape, mask.min(), mask.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2.]), array([55274187,     7297,    14516]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique values in the mask\n",
    "\n",
    "np.unique(mask, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get value distribution of the mask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(mask.flatten(), bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 480, 240)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use matplotlib to show the image\n",
    " \n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 480, 240, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(mask, axis=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_inference_hector import Hector_Dataset_segmentation_emb\n",
    "\n",
    "hect_dataset = Hector_Dataset_segmentation_emb(data_folder = '/share/sda/mohammadqazi/project/hector/pre_processed/',\n",
    "            emd_path = \"/share/sda/mohammadqazi/project/CTscan_prognosis_VLM-main/docs/embeddings/segmentation.npy\",  \n",
    "            csv_file =\"/share/sda/mohammadqazi/project/CTscan_prognosis_VLM-main/docs/TNM_hector_prompts.csv\")\n",
    "\n",
    "train_dataset, test_dataset = hect_dataset.train_val_split(fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_inference_hector.Hector_Dataset_segmentation_emb_subset at 0x7fcdb3501db0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1728, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch\n",
    "model = nn.TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "model(torch.randn(10, 1728, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 512, 24, 24, 24])\n",
      "Output shape: torch.Size([1, 512, 12, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "class Downsample3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Downsample3D, self).__init__()\n",
    "        # Use a kernel size of 3, stride=2, and padding=1 to halve the spatial dimensions.\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is expected to have shape (batch, channels, D, H, W)\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a dummy input tensor of shape (batch, channels, 24, 24, 24)\n",
    "    x = torch.randn(1, 512, 24, 24, 24)\n",
    "    # Initialize the downsampling module; here we keep the same number of channels.\n",
    "    downsample = Downsample3D(in_channels=512, out_channels=512)\n",
    "    # Apply the downsampling\n",
    "    y = downsample(x)\n",
    "    print(\"Input shape: \", x.shape)\n",
    "    print(\"Output shape:\", y.shape)\n",
    "    # Expected output shape: (1, 512, 12, 12, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "class CLIPSeg3DDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPSeg3DDecoder, self).__init__()\n",
    "\n",
    "        self.reduces = nn.ModuleList([nn.Linear(512, 128) for _ in range(4)])\n",
    "        self.blocks = nn.ModuleList([nn.TransformerEncoderLayer(d_model=128, nhead=4) for _ in range(4)])\n",
    "        self.trans_conv = nn.ConvTranspose3d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.downsample = nn.ModuleList([Downsample3D(in_channels=512, out_channels=512) for _ in range(4)])\n",
    "\n",
    "    def forward(self, activations):\n",
    "        \n",
    "        bs = 1  \n",
    "        # # Save the first activation (typically the CLS token output)\n",
    "        # activation1 = activations[0]\n",
    "        # # Use the rest of the activations (we have 3 in your case, since 4 total - 1 = 3)\n",
    "        # activations = activations[1:]\n",
    "\n",
    "        a = None\n",
    "        for i, (activation, block, reduce, downsample) in enumerate(zip(activations, self.blocks, self.reduces, self.downsample)):\n",
    "            if a is not None:\n",
    "                a = reduce(downsample(activation).reshape(bs, 1728, 512)) + a\n",
    "            else:\n",
    "                a = reduce(downsample(activation).reshape(bs, 1728, 512))\n",
    "\n",
    "            a = block(a)\n",
    "        # print(a.shape)\n",
    "\n",
    "        # Remove the CLS token.\n",
    "        # a has shape (bs, C, 1+N) where N=1728. Remove the first token along the token dimension.\n",
    "        # a = a[:, :, 1:]  # shape becomes (bs, C, 1728)\n",
    "\n",
    "        # For 3D, permute so that we have (bs, C, tokens)\n",
    "        # Then compute cube root of token count to reshape into (bs, C, D, H, W)\n",
    "        num_tokens = a.shape[1]  # should be 1728\n",
    "        cube_side = int(round(num_tokens ** (1/3)))  # e.g. 12\n",
    "        # print(num_tokens, cube_side)\n",
    "        # Reshape: assume tokens are in the last dimension\n",
    "        a = a.view(bs, a.shape[2], cube_side, cube_side, cube_side)  # (bs, C, 12, 12, 12)\n",
    "\n",
    "        # print(a.shape)\n",
    "\n",
    "        # Apply 3D transposed convolution to upsample or refine features\n",
    "        a = self.trans_conv(a)\n",
    "\n",
    "        inp_image = torch.randn(1, 1, 240, 480, 480)\n",
    "        # If n_tokens is set, upsample using trilinear interpolation to match input spatial dims\n",
    "        a = nnf.interpolate(a, size=inp_image.shape[2:], mode='trilinear', align_corners=True)\n",
    "\n",
    "        return a\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # inp = torch.randn(1, 1, 240, 480, 480)\n",
    "    \n",
    "    # Instantiate decoder with 3 blocks (for 3 activations)\n",
    "    decoder = CLIPSeg3DDecoder()\n",
    "    \n",
    "    # Forward pass (without conditional input)\n",
    "    # output = decoder.forward([torch.randn(1, 1728, 512) for _ in range(4)])\n",
    "    # print(\"Output shape:\", output[0].shape)  # expected shape: (bs, 1, 240, 480, 480) after interpolation\n",
    "\n",
    "\n",
    "    summary(decoder, input_data=[[torch.randn(1, 512, 24, 24, 24) for _ in range(4)]], depth=8, col_names=[\"input_size\", \"output_size\", \"num_params\", 'trainable'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1728, 128]) torch.Size([1, 1728, 512])\n"
     ]
    }
   ],
   "source": [
    "reduce_1 = nn.Linear(512, 128)\n",
    "reduce_2 = nn.Linear(1728, 1024)\n",
    "reduce_3 = nn.Linear(1728, 1024)\n",
    "reduce_4 = nn.Linear(1728, 1024)\n",
    "\n",
    "block_1 = nn.TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "\n",
    "print(reduce_1(torch.randn(1, 1728, 512)).shape, block_1(torch.randn(1, 1728, 512)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = block_1(torch.randn(1, 512, 1024))\n",
    "num_tokens = a.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "CLIPSeg3DDecoder                         [1, 512, 24, 24, 24]      [1, 1, 240, 480, 480]     --                        True\n",
       "ModuleList: 1-10                       --                        --                        (recursive)               True\n",
       "    Downsample3D: 2-1                 [1, 512, 24, 24, 24]      [1, 512, 12, 12, 12]      --                        True\n",
       "        Conv3d: 3-1                  [1, 512, 24, 24, 24]      [1, 512, 12, 12, 12]      7,078,400                 True\n",
       "        BatchNorm3d: 3-2             [1, 512, 12, 12, 12]      [1, 512, 12, 12, 12]      1,024                     True\n",
       "        ReLU: 3-3                    [1, 512, 12, 12, 12]      [1, 512, 12, 12, 12]      --                        --\n",
       "ModuleList: 1-11                       --                        --                        (recursive)               True\n",
       "    Linear: 2-2                       [1, 1728, 512]            [1, 1728, 128]            65,664                    True\n",
       "ModuleList: 1-12                       --                        --                        (recursive)               True\n",
       "    TransformerEncoderLayer: 2-3      [1, 1728, 128]            [1, 1728, 128]            --                        True\n",
       "        MultiheadAttention: 3-4      [1, 1728, 128]            [1, 1728, 128]            66,048                    True\n",
       "        Dropout: 3-5                 [1, 1728, 128]            [1, 1728, 128]            --                        --\n",
       "        LayerNorm: 3-6               [1, 1728, 128]            [1, 1728, 128]            256                       True\n",
       "        Linear: 3-7                  [1, 1728, 128]            [1, 1728, 2048]           264,192                   True\n",
       "        Dropout: 3-8                 [1, 1728, 2048]           [1, 1728, 2048]           --                        --\n",
       "        Linear: 3-9                  [1, 1728, 2048]           [1, 1728, 128]            262,272                   True\n",
       "        Dropout: 3-10                [1, 1728, 128]            [1, 1728, 128]            --                        --\n",
       "        LayerNorm: 3-11              [1, 1728, 128]            [1, 1728, 128]            256                       True\n",
       "ModuleList: 1-10                       --                        --                        (recursive)               True\n",
       "    Downsample3D: 2-4                 [1, 512, 24, 24, 24]      [1, 512, 12, 12, 12]      --                        True\n",
       "        Conv3d: 3-12                 [1, 512, 24, 24, 24]      [1, 512, 12, 12, 12]      7,078,400                 True\n",
       "        BatchNorm3d: 3-13            [1, 512, 12, 12, 12]      [1, 512, 12, 12, 12]      1,024                     True\n",
       "        ReLU: 3-14                   [1, 512, 12, 12, 12]      [1, 512, 12, 12, 12]      --                        --\n",
       "ModuleList: 1-11                       --                        --                        (recursive)               True\n",
       "    Linear: 2-5                       [1, 1728, 512]            [1, 1728, 128]            65,664                    True\n",
       "ModuleList: 1-12                       --                        --                        (recursive)               True\n",
       "    TransformerEncoderLayer: 2-6      [1, 1728, 128]            [1, 1728, 128]            --                        True\n",
       "        MultiheadAttention: 3-15     [1, 1728, 128]            [1, 1728, 128]            66,048                    True\n",
       "        Dropout: 3-16                [1, 1728, 128]            [1, 1728, 128]            --                        --\n",
       "        LayerNorm: 3-17              [1, 1728, 128]            [1, 1728, 128]            256                       True\n",
       "        Linear: 3-18                 [1, 1728, 128]            [1, 1728, 2048]           264,192                   True\n",
       "        Dropout: 3-19                [1, 1728, 2048]           [1, 1728, 2048]           --                        --\n",
       "        Linear: 3-20                 [1, 1728, 2048]           [1, 1728, 128]            262,272                   True\n",
       "        Dropout: 3-21                [1, 1728, 128]            [1, 1728, 128]            --                        --\n",
       "        LayerNorm: 3-22              [1, 1728, 128]            [1, 1728, 128]            256                       True\n",
       "ModuleList: 1-10                       --                        --                        (recursive)               True\n",
       "    Downsample3D: 2-7                 [1, 512, 24, 24, 24]      [1, 512, 12, 12, 12]      --                        True\n",
       "        Conv3d: 3-23                 [1, 512, 24, 24, 24]      [1, 512, 12, 12, 12]      7,078,400                 True\n",
       "        BatchNorm3d: 3-24            [1, 512, 12, 12, 12]      [1, 512, 12, 12, 12]      1,024                     True\n",
       "        ReLU: 3-25                   [1, 512, 12, 12, 12]      [1, 512, 12, 12, 12]      --                        --\n",
       "ModuleList: 1-11                       --                        --                        (recursive)               True\n",
       "    Linear: 2-8                       [1, 1728, 512]            [1, 1728, 128]            65,664                    True\n",
       "ModuleList: 1-12                       --                        --                        (recursive)               True\n",
       "    TransformerEncoderLayer: 2-9      [1, 1728, 128]            [1, 1728, 128]            --                        True\n",
       "        MultiheadAttention: 3-26     [1, 1728, 128]            [1, 1728, 128]            66,048                    True\n",
       "        Dropout: 3-27                [1, 1728, 128]            [1, 1728, 128]            --                        --\n",
       "        LayerNorm: 3-28              [1, 1728, 128]            [1, 1728, 128]            256                       True\n",
       "        Linear: 3-29                 [1, 1728, 128]            [1, 1728, 2048]           264,192                   True\n",
       "        Dropout: 3-30                [1, 1728, 2048]           [1, 1728, 2048]           --                        --\n",
       "        Linear: 3-31                 [1, 1728, 2048]           [1, 1728, 128]            262,272                   True\n",
       "        Dropout: 3-32                [1, 1728, 128]            [1, 1728, 128]            --                        --\n",
       "        LayerNorm: 3-33              [1, 1728, 128]            [1, 1728, 128]            256                       True\n",
       "ModuleList: 1-10                       --                        --                        (recursive)               True\n",
       "    Downsample3D: 2-10                [1, 512, 24, 24, 24]      [1, 512, 12, 12, 12]      --                        True\n",
       "        Conv3d: 3-34                 [1, 512, 24, 24, 24]      [1, 512, 12, 12, 12]      7,078,400                 True\n",
       "        BatchNorm3d: 3-35            [1, 512, 12, 12, 12]      [1, 512, 12, 12, 12]      1,024                     True\n",
       "        ReLU: 3-36                   [1, 512, 12, 12, 12]      [1, 512, 12, 12, 12]      --                        --\n",
       "ModuleList: 1-11                       --                        --                        (recursive)               True\n",
       "    Linear: 2-11                      [1, 1728, 512]            [1, 1728, 128]            65,664                    True\n",
       "ModuleList: 1-12                       --                        --                        (recursive)               True\n",
       "    TransformerEncoderLayer: 2-12     [1, 1728, 128]            [1, 1728, 128]            --                        True\n",
       "        MultiheadAttention: 3-37     [1, 1728, 128]            [1, 1728, 128]            66,048                    True\n",
       "        Dropout: 3-38                [1, 1728, 128]            [1, 1728, 128]            --                        --\n",
       "        LayerNorm: 3-39              [1, 1728, 128]            [1, 1728, 128]            256                       True\n",
       "        Linear: 3-40                 [1, 1728, 128]            [1, 1728, 2048]           264,192                   True\n",
       "        Dropout: 3-41                [1, 1728, 2048]           [1, 1728, 2048]           --                        --\n",
       "        Linear: 3-42                 [1, 1728, 2048]           [1, 1728, 128]            262,272                   True\n",
       "        Dropout: 3-43                [1, 1728, 128]            [1, 1728, 128]            --                        --\n",
       "        LayerNorm: 3-44              [1, 1728, 128]            [1, 1728, 128]            256                       True\n",
       "ConvTranspose3d: 1-13                  [1, 128, 12, 12, 12]      [1, 1, 12, 12, 12]        3,457                     True\n",
       "============================================================================================================================================\n",
       "Total params: 30,955,905\n",
       "Trainable params: 30,955,905\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 48.93\n",
       "============================================================================================================================================\n",
       "Input size (MB): 113.25\n",
       "Forward/backward pass size (MB): 198.19\n",
       "Params size (MB): 122.77\n",
       "Estimated Total Size (MB): 434.21\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = CLIPSeg3DDecoder(\n",
    "    n_tokens=1728,  # example setting\n",
    ")\n",
    "\n",
    "summary(decoder, input_data=[[torch.randn(1, 512, 24, 24, 24) for _ in range(4)]], depth=8, col_names=[\"input_size\", \"output_size\", \"num_params\", 'trainable'],)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct_rate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
