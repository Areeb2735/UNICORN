{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_inference_hector import Hector_Dataset_ct_pt\n",
    "\n",
    "hect_dataset = Hector_Dataset_ct_pt(data_folder = \"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/valid_preprocessed_hector/\",  \n",
    "                csv_file =\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/TNM_hector_prompts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 240, 480, 480]),\n",
       " torch.Size([1, 240, 480, 480]),\n",
       " \"Patient Information and Clinical Summary:\\n\\nThe patient is an 82-year-old male with a weight of 80.0 kg. Information regarding the patient's alcohol consumption and performance status is not available. The patient's HPV status is also not specified. The patient has undergone chemotherapy. There is no available information about any surgical interventions.\\n\\nTNM Staging:\\n\\nAccording to the 7th edition of the TNM staging system, the patient is classified as T2, N2, M0, which corresponds to a TNM group IV. This indicates a locally advanced disease with regional lymph node involvement but no distant metastasis.\\n\\nConclusion:\\n\\nIn summary, this is an 82-year-old male patient with a history of chemotherapy treatment for a cancer classified as T2N2M0, TNM group IV, according to the 7th edition of the TNM staging system. Further information regarding the patient's alcohol consumption, performance status, HPV status, and surgical history is required for a more comprehensive assessment.\",\n",
       " 0,\n",
       " 1704,\n",
       " 'CHUM-001_ct_roi.npz')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hect_dataset[0][0].shape, hect_dataset[0][1].shape, hect_dataset[0][2], hect_dataset[0][3], hect_dataset[0][4], hect_dataset[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:391: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT_CLIP/ct_clip/ct_clip.py:598: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pt = torch.load(str(path))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam, AdamW\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils import make_time_bins\n",
    "from utils import encode_survival, mtlr_neg_log_likelihood, make_optimizer\n",
    "from utils import mtlr_survival, mtlr_risk, roc_auc_at_times, brier_score_at_times\n",
    "from prognosis_model import model_ctpt\n",
    "\n",
    "from ct_clip import CTCLIP\n",
    "from transformer_maskgit import CTViT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from lifelines.utils import concordance_index\n",
    "from data_inference_hector import Hector_Dataset_emb, Hector_Dataset\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed) \n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized',do_lower_case=True)\n",
    "text_encoder = BertModel.from_pretrained(\"microsoft/BiomedVLP-CXR-BERT-specialized\")\n",
    "\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "text_encoder.to(device)\n",
    "\n",
    "image_encoder = CTViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 8192,\n",
    "    image_size = 480,\n",
    "    patch_size = 20,\n",
    "    temporal_patch_size = 10,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 32,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "image_encoder.to(device)\n",
    "\n",
    "clip = CTCLIP(\n",
    "    image_encoder = image_encoder,\n",
    "    text_encoder = text_encoder,\n",
    "    dim_image = 294912,\n",
    "    dim_text = 768,\n",
    "    dim_latent = 512,\n",
    "    extra_latent_projection = False,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)\n",
    "    use_mlm=False,\n",
    "    downsample_image_embeds = False,\n",
    "    use_all_token_embeds = False,\n",
    ")\n",
    "\n",
    "clip.load(\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT-CLIP_v2.pt\")\n",
    "clip.to(device)\n",
    "\n",
    "num_time_bins = 12\n",
    "\n",
    "model = model_ctpt(clip, device, num_time_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize new layers manually\n",
    "def initialize_weights(layer):\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    elif isinstance(layer, nn.LayerNorm):\n",
    "        nn.init.ones_(layer.weight)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "\n",
    "model.clip.visual_transformer.to_patch_emb_pt.apply(initialize_weights)\n",
    "model.clip.visual_transformer.merge_modalities.apply(initialize_weights)\n",
    "\n",
    "# Step 4: Freeze CT-specific layers\n",
    "for name, param in model.named_parameters():\n",
    "    if 'to_patch_emb_pt' in name or 'merge_modalities' in name:\n",
    "        param.requires_grad = True  # Train these layers\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze all other layers\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(x in name for x in [\"img_embd\", \"text_embd\", \"fuse\", \"mtlr\"]):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip.visual_transformer.to_patch_emb_pt.1.weight: requires_grad=True\n",
      "clip.visual_transformer.to_patch_emb_pt.1.bias: requires_grad=True\n",
      "clip.visual_transformer.to_patch_emb_pt.2.weight: requires_grad=True\n",
      "clip.visual_transformer.to_patch_emb_pt.2.bias: requires_grad=True\n",
      "clip.visual_transformer.to_patch_emb_pt.3.weight: requires_grad=True\n",
      "clip.visual_transformer.to_patch_emb_pt.3.bias: requires_grad=True\n",
      "clip.visual_transformer.merge_modalities.conv.weight: requires_grad=True\n",
      "clip.visual_transformer.merge_modalities.conv.bias: requires_grad=True\n",
      "clip.visual_transformer.merge_modalities.norm.weight: requires_grad=True\n",
      "clip.visual_transformer.merge_modalities.norm.bias: requires_grad=True\n",
      "img_embd.0.weight: requires_grad=True\n",
      "img_embd.0.bias: requires_grad=True\n",
      "img_embd.2.weight: requires_grad=True\n",
      "img_embd.2.bias: requires_grad=True\n",
      "img_embd.3.weight: requires_grad=True\n",
      "img_embd.3.bias: requires_grad=True\n",
      "text_embd.0.weight: requires_grad=True\n",
      "text_embd.0.bias: requires_grad=True\n",
      "text_embd.2.weight: requires_grad=True\n",
      "text_embd.2.bias: requires_grad=True\n",
      "text_embd.3.weight: requires_grad=True\n",
      "text_embd.3.bias: requires_grad=True\n",
      "fuse.0.weight: requires_grad=True\n",
      "fuse.0.bias: requires_grad=True\n",
      "fuse.2.weight: requires_grad=True\n",
      "fuse.2.bias: requires_grad=True\n",
      "mtlr.mtlr_weight: requires_grad=True\n",
      "mtlr.mtlr_bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
