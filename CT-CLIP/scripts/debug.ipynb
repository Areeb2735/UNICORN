{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:391: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT_CLIP/ct_clip/ct_clip.py:598: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pt = torch.load(str(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CTCLIP(\n",
       "  (text_transformer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.25, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (visual_transformer): CTViT(\n",
       "    (spatial_rel_pos_bias): ContinuousPositionBias(\n",
       "      (net): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "        (2): Linear(in_features=512, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (to_patch_emb_first_frame): Sequential(\n",
       "      (0): Rearrange('b c 1 (h p1) (w p2) -> b 1 h w (c p1 p2)', p1=20, p2=20)\n",
       "      (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=400, out_features=512, bias=True)\n",
       "      (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (to_patch_emb): Sequential(\n",
       "      (0): Rearrange('b c (t pt) (h p1) (w p2) -> b t h w (c pt p1 p2)', p1=20, p2=20, pt=10)\n",
       "      (1): LayerNorm((4000,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=4000, out_features=512, bias=True)\n",
       "      (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_spatial_transformer): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ModuleList(\n",
       "          (0): PEG(\n",
       "            (dsconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=512)\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "            (context_norm): LayerNorm()\n",
       "            (to_q): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_out): Linear(in_features=256, out_features=512, bias=False)\n",
       "          )\n",
       "          (2): None\n",
       "          (3): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=2730, bias=False)\n",
       "            (2): GEGLU()\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=1365, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): LayerNorm()\n",
       "    )\n",
       "    (enc_temporal_transformer): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ModuleList(\n",
       "          (0): PEG(\n",
       "            (dsconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=512)\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "            (context_norm): LayerNorm()\n",
       "            (to_q): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_out): Linear(in_features=256, out_features=512, bias=False)\n",
       "          )\n",
       "          (2): None\n",
       "          (3): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=2730, bias=False)\n",
       "            (2): GEGLU()\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=1365, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): LayerNorm()\n",
       "    )\n",
       "    (vq): VectorQuantize(\n",
       "      (project_in): Identity()\n",
       "      (project_out): Identity()\n",
       "      (_codebook): CosineSimCodebook()\n",
       "    )\n",
       "    (to_pixels_first_frame): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=400, bias=True)\n",
       "      (1): Rearrange('b 1 h w (c p1 p2) -> b c 1 (h p1) (w p2)', p1=20, p2=20)\n",
       "    )\n",
       "    (to_pixels): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=4000, bias=True)\n",
       "      (1): Rearrange('b t h w (c pt p1 p2) -> b c (t pt) (h p1) (w p2)', p1=20, p2=20, pt=10)\n",
       "    )\n",
       "  )\n",
       "  (to_text_latent): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (to_visual_latent): Linear(in_features=294912, out_features=512, bias=False)\n",
       "  (to_text_latent_extra): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (to_visual_latent_extra): Linear(in_features=294912, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prognosis_model import embd_model, lora_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam, AdamW\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils import make_time_bins\n",
    "from utils import encode_survival, mtlr_neg_log_likelihood, make_optimizer\n",
    "from utils import mtlr_survival, mtlr_risk, roc_auc_at_times, brier_score_at_times\n",
    "from prognosis_model import embd_model, lora_model\n",
    "\n",
    "from ct_clip import CTCLIP\n",
    "from transformer_maskgit import CTViT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from lifelines.utils import concordance_index\n",
    "from data_inference_hector import Hector_Dataset_emb, Hector_Dataset\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed) \n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized',do_lower_case=True)\n",
    "text_encoder = BertModel.from_pretrained(\"microsoft/BiomedVLP-CXR-BERT-specialized\")\n",
    "\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "text_encoder.to(device)\n",
    "\n",
    "image_encoder = CTViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 8192,\n",
    "    image_size = 480,\n",
    "    patch_size = 20,\n",
    "    temporal_patch_size = 10,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 32,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "image_encoder.to(device)\n",
    "\n",
    "clip = CTCLIP(\n",
    "    image_encoder = image_encoder,\n",
    "    text_encoder = text_encoder,\n",
    "    dim_image = 294912,\n",
    "    dim_text = 768,\n",
    "    dim_latent = 512,\n",
    "    extra_latent_projection = False,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)\n",
    "    use_mlm=False,\n",
    "    downsample_image_embeds = False,\n",
    "    use_all_token_embeds = False,\n",
    ")\n",
    "\n",
    "clip.load(\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT-CLIP_v2.pt\")\n",
    "clip.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emb_gen(nn.Module):\n",
    "    def __init__(self, clip, device, num_time_bins):\n",
    "        super(emb_gen, self).__init__()\n",
    "        self.clip = clip\n",
    "        self.device = device\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.img_embd = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "        )\n",
    "        self.text_embd = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "        )\n",
    "\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=512 * 2,\n",
    "                out_channels=self.hidden_dim,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.hidden_dim,\n",
    "                out_channels=self.hidden_dim,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "        )\n",
    "        self.mtlr = MTLR(in_features=512, num_time_bins=num_time_bins)\n",
    "\n",
    "\n",
    "    def forward(self, text, image):\n",
    "        self.clip.eval()\n",
    "        with torch.no_grad():\n",
    "            img, text = self.clip(text, image, self.device, prognosis = True)\n",
    "    \n",
    "        img = self.avgpool(img.permute(0, 3, 1, 2)).squeeze(-1).squeeze(-1)\n",
    "        text = text.mean(dim=1)\n",
    "\n",
    "        img = self.img_embd(img)\n",
    "        text = self.text_embd(text)\n",
    "\n",
    "        fuse = torch.cat([img, text], dim=1) \n",
    "        fuse = self.fuse(fuse.unsqueeze(2))\n",
    "        pred = self.mtlr(fuse.squeeze(2))\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=88\n",
    "text_emb=tokenizer(hect_dataset[n][1], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(device)\n",
    "clip.eval()\n",
    "with torch.no_grad():\n",
    "    emb = clip(text_emb, hect_dataset[n][0].unsqueeze(0).cuda(), device, prognosis = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embd = nn.Sequential(\n",
    "    nn.Linear(512, 512),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.LayerNorm(512),\n",
    ")\n",
    "text_embd = nn.Sequential(\n",
    "    nn.Linear(768, 512),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.LayerNorm(512),\n",
    ")\n",
    "\n",
    "fuse = nn.Sequential(\n",
    "    nn.Conv1d(\n",
    "        in_channels=512 * 2,\n",
    "        out_channels=512,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "    ),\n",
    "    nn.GELU(),\n",
    "    nn.Conv1d(\n",
    "        in_channels=512,\n",
    "        out_channels=512,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "img_embd = img_embd.to(device)\n",
    "text_embd = text_embd.to(device)\n",
    "fuse = fuse.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0].shape, emb[1].shape\n",
    "\n",
    "img = avgpool(emb[0].permute(0, 3, 1, 2)).squeeze(-1).squeeze(-1)\n",
    "text = emb[1].mean(dim=1)\n",
    "\n",
    "img = img_embd(img)\n",
    "text = text_embd(text)\n",
    "\n",
    "feat = torch.cat([img, text], dim=1) \n",
    "\n",
    "fuse(feat.unsqueeze(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]), torch.Size([1, 1024, 1]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, text.shape, feat.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/TNM_hector_prompts.csv\")\n",
    "\n",
    "num_time_bins = 12\n",
    "time_bins = make_time_bins(df['RFS'].values, event=df['Relapse'].values, num_bins=num_time_bins)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    inference_mode=False, r=8, lora_alpha=64, lora_dropout=0.2, target_modules=[\"to_q\", \"to_kv\"]\n",
    ")\n",
    "\n",
    "model = lora_model(clip, device, peft_config, num_time_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_inference_hector import Hector_Dataset_emb, Hector_Dataset\n",
    "\n",
    "hect_dataset = Hector_Dataset(data_folder = \"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/valid_preprocessed_hector/\",  \n",
    "                csv_file =\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/TNM_hector_prompts.csv\")\n",
    "\n",
    "hect_dataset_emb = Hector_Dataset_emb(emd_path = '/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/embeddings_new_TNM.npy',  \n",
    "                csv_file =\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/TNM_hector_prompts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409, 409)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hect_dataset), len(hect_dataset_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CHUM-055_ct_roi.npz', 'CHUM-055_ct_roi.npz')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hect_dataset[45][4], hect_dataset_emb[45][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(-1.))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hect_dataset[0][0].max(), hect_dataset[0][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1154), tensor(-0.1333))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hect_dataset_emb[0][0].max(), hect_dataset_emb[0][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Patient Information and Clinical Summary:\\n\\nThe patient is an 82-year-old male with a weight of 80.0 kg. Information regarding the patient's alcohol consumption and performance status is not available. The patient's HPV status is also not specified. The patient has undergone chemotherapy. There is no available information about any surgical interventions.\\n\\nTNM Staging:\\n\\nAccording to the 7th edition of the TNM staging system, the patient is classified as T2, N2, M0, which corresponds to a TNM group IV. This indicates a locally advanced disease with regional lymph node involvement but no distant metastasis.\\n\\nConclusion:\\n\\nIn summary, this is an 82-year-old male patient with a history of chemotherapy treatment for a cancer classified as T2N2M0, TNM group IV, according to the 7th edition of the TNM staging system. Further information regarding the patient's alcohol consumption, performance status, HPV status, and surgical history is required for a more comprehensive assessment.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hect_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 240, 480, 480])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hect_dataset[0][0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13824, 512]) tensor(14.8312, device='cuda:0') tensor(-10.7521, device='cuda:0')\n",
      "after torch.Size([1, 13824, 512]) tensor(0.8041, device='cuda:0') tensor(-0.7761, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 24, 24, 24, 512]),\n",
       " tensor(0.8041, device='cuda:0'),\n",
       " tensor(-0.7761, device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoder.eval()\n",
    "with torch.no_grad():\n",
    "    encd = image_encoder(hect_dataset[n][0].unsqueeze(0).to(device), return_encoded_tokens=True)\n",
    "encd.shape, encd.max(), encd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13824, 512]) tensor(14.8312, device='cuda:0') tensor(-10.7521, device='cuda:0')\n",
      "after torch.Size([1, 13824, 512]) tensor(0.8041, device='cuda:0') tensor(-0.7761, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 24, 24, 24, 512]),\n",
       " tensor(0.8041, device='cuda:0'),\n",
       " tensor(-0.7761, device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encd = image_encoder(hect_dataset[n][0].unsqueeze(0).cuda(), return_encoded_tokens=True)\n",
    "encd.shape, encd.max(), encd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 24, 24, 24, 512]),\n",
       " tensor(0.7667, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(-0.7625, device='cuda:0', grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encd = image_encoder(hect_dataset[n][0].unsqueeze(0).cuda(), return_encoded_tokens=True)\n",
    "encd.shape, encd.max(), encd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13824, 512]) tensor(14.8312, device='cuda:0') tensor(-10.7521, device='cuda:0')\n",
      "after torch.Size([1, 13824, 512]) tensor(0.8041, device='cuda:0') tensor(-0.7761, device='cuda:0')\n",
      "text embd torch.Size([1, 768]) tensor(1.2392, device='cuda:0') tensor(-1.0307, device='cuda:0')\n",
      "image embd torch.Size([1, 294912]) tensor(0.4192, device='cuda:0') tensor(-0.3291, device='cuda:0')\n",
      "text latents torch.Size([1, 512]) tensor(0.1407, device='cuda:0') tensor(-0.1061, device='cuda:0')\n",
      "image latents torch.Size([1, 512]) tensor(0.1090, device='cuda:0') tensor(-0.1142, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]),\n",
       " torch.Size([1, 512]),\n",
       " tensor(0.1090, device='cuda:0'),\n",
       " tensor(-0.1142, device='cuda:0'),\n",
       " tensor(0.1407, device='cuda:0'),\n",
       " tensor(-0.1061, device='cuda:0'),\n",
       " 'HMR-034_ct_roi.npz')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emb=tokenizer(hect_dataset[n][1], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(device)\n",
    "clip.eval()\n",
    "with torch.no_grad():\n",
    "    emb = clip(text_emb, hect_dataset[n][0].unsqueeze(0).cuda(), device, embed = True)\n",
    "\n",
    "emb[0].shape, emb[1].shape, emb[0].max(), emb[0].min(), emb[1].max(), emb[1].min(), hect_dataset[n][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13824, 512]) tensor(14.8312, device='cuda:0') tensor(-10.7521, device='cuda:0')\n",
      "after torch.Size([1, 13824, 512]) tensor(0.8041, device='cuda:0') tensor(-0.7761, device='cuda:0')\n",
      "text embd torch.Size([1, 768]) tensor(1.2392, device='cuda:0') tensor(-1.0307, device='cuda:0')\n",
      "image embd torch.Size([1, 294912]) tensor(0.4192, device='cuda:0') tensor(-0.3291, device='cuda:0')\n",
      "text latents torch.Size([1, 512]) tensor(0.1407, device='cuda:0') tensor(-0.1061, device='cuda:0')\n",
      "image latents torch.Size([1, 512]) tensor(0.1090, device='cuda:0') tensor(-0.1142, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]),\n",
       " torch.Size([1, 512]),\n",
       " tensor(0.1090, device='cuda:0'),\n",
       " tensor(-0.1142, device='cuda:0'),\n",
       " tensor(0.1407, device='cuda:0'),\n",
       " tensor(-0.1061, device='cuda:0'),\n",
       " 'HMR-034_ct_roi.npz')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emb=tokenizer(hect_dataset[n][1], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(device)\n",
    "clip.eval()\n",
    "with torch.no_grad():\n",
    "    emb = clip(text_emb, hect_dataset[n][0].unsqueeze(0).cuda(), device, embed = True)\n",
    "\n",
    "emb[0].shape, emb[1].shape, emb[0].max(), emb[0].min(), emb[1].max(), emb[1].min(), hect_dataset[n][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512]),\n",
       " torch.Size([512]),\n",
       " tensor(0.1090),\n",
       " tensor(-0.1142),\n",
       " tensor(0.1407),\n",
       " tensor(-0.1061))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hect_dataset_emb[n][0].shape, hect_dataset_emb[n][1].shape, hect_dataset_emb[n][0].max(), hect_dataset_emb[n][0].min(), hect_dataset_emb[n][1].max(), hect_dataset_emb[n][1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_inference_hector import Hector_Dataset_emb\n",
    "\n",
    "hect_dataset = Hector_Dataset_emb(emd_path = '/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/embeddings_new_exp_.npy',  \n",
    "                csv_file =\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/TNM_hector_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/TNM_hector_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "\n",
    "# Shuffle indices\n",
    "num_rows = len(df)\n",
    "folds = np.tile(np.arange(num_folds), num_rows // num_folds + 1)[:num_rows]  \n",
    "np.random.shuffle(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]), array([82, 82, 82, 82, 81]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  get the number of samples in each fold\n",
    "\n",
    "np.unique(folds, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PatientID Gender  Age  Weight  Tobacco  Alcohol  Performance status  \\\n",
      "0    CHUM-001      M   82    80.0      NaN      NaN                 NaN   \n",
      "1    CHUM-002      M   73    55.0      NaN      NaN                 NaN   \n",
      "2    CHUM-006      M   65   101.0      NaN      NaN                 NaN   \n",
      "3    CHUM-007      F   70    80.0      NaN      NaN                 NaN   \n",
      "4    CHUM-008      F   67    91.0      NaN      NaN                 NaN   \n",
      "..        ...    ...  ...     ...      ...      ...                 ...   \n",
      "404   HMR-028      M   73    87.0      NaN      NaN                 NaN   \n",
      "405   HMR-029      M   57     NaN      NaN      NaN                 NaN   \n",
      "406   HMR-030      M   70     NaN      NaN      NaN                 NaN   \n",
      "407   HMR-034      F   85     NaN      NaN      NaN                 NaN   \n",
      "408   HMR-040      F   61    53.0      NaN      NaN                 NaN   \n",
      "\n",
      "     HPV status (0=-, 1=+)  Surgery  Chemotherapy  Relapse   RFS  TNM edition  \\\n",
      "0                      NaN      NaN             1        0  1704          7.0   \n",
      "1                      NaN      NaN             1        1   439          7.0   \n",
      "2                      NaN      NaN             1        0  1186          7.0   \n",
      "3                      NaN      NaN             0        0  1702          7.0   \n",
      "4                      NaN      NaN             1        0  1499          7.0   \n",
      "..                     ...      ...           ...      ...   ...          ...   \n",
      "404                    NaN      NaN             1        0   419          7.0   \n",
      "405                    1.0      NaN             1        0  1736          7.0   \n",
      "406                    NaN      NaN             1        0  1385          7.0   \n",
      "407                    NaN      NaN             1        0  1570          7.0   \n",
      "408                    NaN      NaN             1        0  1354          7.0   \n",
      "\n",
      "    T-stage N-stage M-stage TNM group  \\\n",
      "0        T2      N2      M0        IV   \n",
      "1        T3      N2      M0        IV   \n",
      "2        T2      N2      M0        IV   \n",
      "3        T2      N2      M0        IV   \n",
      "4        T2      N2      M0        IV   \n",
      "..      ...     ...     ...       ...   \n",
      "404      T4     N2c      M0       IVA   \n",
      "405      T2     N2b      M0       IVA   \n",
      "406      T2     N2b      M0       IVA   \n",
      "407      T2     N2b      M0       IVA   \n",
      "408      T4     N2c      M0       IVA   \n",
      "\n",
      "                                                  text  fold  \n",
      "0    Patient Information and Clinical Summary:\\n\\nT...     0  \n",
      "1    Patient Information and Clinical Observations:...     2  \n",
      "2    Patient Radiology Report:\\n\\nThe patient is a ...     4  \n",
      "3    Patient Information and Clinical Summary:\\n\\nT...     1  \n",
      "4    Patient Information and Clinical Summary:\\n\\nT...     1  \n",
      "..                                                 ...   ...  \n",
      "404  Patient Information and Clinical Observations:...     3  \n",
      "405  Patient Information and Clinical Observations:...     0  \n",
      "406  Patient Radiology Report:\\n\\nPatient Informati...     2  \n",
      "407  Patient Information and Clinical Observations:...     4  \n",
      "408  Patient Information and Clinical Summary:\\n\\nT...     3  \n",
      "\n",
      "[409 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assign to DataFrame\n",
    "df['fold'] = folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    82\n",
       "2    82\n",
       "1    82\n",
       "3    82\n",
       "4    81\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fold'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/TNM_hector_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_inference_hector import Hector_Dataset_emb\n",
    "\n",
    "\n",
    "hect_dataset = Hector_Dataset_emb(emd_path = '/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/embeddings_new_exp_.npy',  \n",
    "                csv_file =\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/TNM_hector_prompts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = hect_dataset.train_val_split(fold=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
