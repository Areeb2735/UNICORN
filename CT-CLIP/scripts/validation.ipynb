{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammadqazi/.conda/envs/ct_rate/lib/python3.10/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/mohammadqazi/.conda/envs/ct_rate/lib/python3.10/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:391: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Cache' from 'transformers' (/home/mohammadqazi/.conda/envs/ct_rate/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlifelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m concordance_index\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata_inference_hector\u001b[39;00m \u001b[39mimport\u001b[39;00m Hector_Dataset_emb, Hector_Dataset\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m get_peft_config, get_peft_model, LoraConfig, TaskType\n\u001b[1;32m     26\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m\n\u001b[1;32m     27\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(seed) \n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/peft/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.14.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     AutoPeftModel,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     26\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     27\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     28\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     29\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     33\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     inject_adapter_in_model,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmixed_model\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftMixedModel\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/peft/auto.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     AutoModel,\n\u001b[1;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     PeftModel,\n\u001b[1;32m     35\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/peft/mapping.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mxlora\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m XLoraModel\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmixed_model\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftMixedModel\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     PeftModel,\n\u001b[1;32m     28\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     36\u001b[0m     AdaLoraConfig,\n\u001b[1;32m     37\u001b[0m     AdaLoraModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     XLoraConfig,\n\u001b[1;32m     72\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/peft/mixed_model.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m DUMMY_MODEL_CONFIG\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     AdaLoraModel,\n\u001b[1;32m     32\u001b[0m     IA3Model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     MixedModel,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPATIBLE_TUNER_TYPES\n",
      "File \u001b[0;32m~/.conda/envs/ct_rate/lib/python3.10/site-packages/peft/peft_model.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msafetensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m save_file \u001b[39mas\u001b[39;00m safe_save_file\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m PushToHubMixin\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Cache' from 'transformers' (/home/mohammadqazi/.conda/envs/ct_rate/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils import make_time_bins\n",
    "from utils import encode_survival, mtlr_neg_log_likelihood, make_optimizer\n",
    "from utils import mtlr_survival, mtlr_risk\n",
    "from prognosis_model import embd_model, lora_model\n",
    "\n",
    "from ct_clip import CTCLIP\n",
    "from transformer_maskgit import CTViT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from lifelines.utils import concordance_index\n",
    "from data_inference_hector import Hector_Dataset_emb, Hector_Dataset\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed) \n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized',do_lower_case=True)\n",
    "text_encoder = BertModel.from_pretrained(\"microsoft/BiomedVLP-CXR-BERT-specialized\")\n",
    "\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "text_encoder.to(device)\n",
    "\n",
    "image_encoder = CTViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 8192,\n",
    "    image_size = 480,\n",
    "    patch_size = 20,\n",
    "    temporal_patch_size = 10,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 32,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "image_encoder.to(device)\n",
    "\n",
    "clip = CTCLIP(\n",
    "    image_encoder = image_encoder,\n",
    "    text_encoder = text_encoder,\n",
    "    dim_image = 294912,\n",
    "    dim_text = 768,\n",
    "    dim_latent = 512,\n",
    "    extra_latent_projection = False,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)\n",
    "    use_mlm=False,\n",
    "    downsample_image_embeds = False,\n",
    "    use_all_token_embeds = False,\n",
    ")\n",
    "\n",
    "clip.load(\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT-CLIP_v2.pt\")\n",
    "clip.to(device)\n",
    "\n",
    "hect_dataset = Hector_Dataset(data_folder = \"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/valid_preprocessed_hector/\",  \n",
    "                csv_file =\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/final_hector_with_text.csv\")\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(hect_dataset))  # 80% for training\n",
    "test_size = len(hect_dataset) - train_size  # 20% for testing\n",
    "train_dataset, test_dataset = random_split(hect_dataset, [train_size, test_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "df = pd.read_csv(\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/final_hector_with_text.csv\")\n",
    "time_bins = make_time_bins(df['RFS'].values, event = df['Relapse'].values)\n",
    "num_time_bins = len(time_bins)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    inference_mode=False, r=8, lora_alpha=64, lora_dropout=0.2, target_modules=[\"to_q\", \"to_kv\"]\n",
    ")\n",
    "\n",
    "model = lora_model(clip, device, peft_config, num_time_bins)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3188212/99424953.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/model_weights/weight_095.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation of the model\n",
      "Concordance Index: 0.4298\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/model_weights/weight_095.pth\"))\n",
    "print(\"Validation of the model\")\n",
    "model.eval()\n",
    "pred_risk_all = []\n",
    "relapse_all = []\n",
    "RFS_all = []\n",
    "pred_survival_all = []\n",
    "with torch.no_grad():\n",
    "    for img_emb, text_emb, relapse, RFS, _ in test_loader:\n",
    "        img_emb = img_emb.to(device)\n",
    "        text_emb=tokenizer(text_emb, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "        y_pred = model(text_emb, img_emb)\n",
    "        pred_survival = mtlr_survival(y_pred).cpu().numpy()\n",
    "        pred_risk = mtlr_risk(y_pred).cpu().numpy()\n",
    "\n",
    "        pred_risk_all.append(pred_risk.item()) \n",
    "        relapse_all.append(relapse.item())\n",
    "        RFS_all.append(RFS.item())\n",
    "        pred_survival_all.append(list(pred_survival[0]))\n",
    "\n",
    "ci = concordance_index(RFS_all, -np.array(pred_risk_all), event_observed=relapse_all)\n",
    "print(f\"Concordance Index: {ci:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.9954841 , 0.98311925, ..., 0.84875095, 0.7843362 ,\n",
       "        0.4419284 ],\n",
       "       [0.99999994, 0.9804237 , 0.9392827 , ..., 0.6399225 , 0.5471909 ,\n",
       "        0.31601977],\n",
       "       [1.        , 0.9896096 , 0.96635306, ..., 0.7328142 , 0.6472257 ,\n",
       "        0.36006597],\n",
       "       ...,\n",
       "       [0.99999994, 0.9957256 , 0.9821786 , ..., 0.7130777 , 0.64634717,\n",
       "        0.29794365],\n",
       "       [1.        , 0.9987982 , 0.99350846, ..., 0.928804  , 0.8949395 ,\n",
       "        0.50034034],\n",
       "       [1.        , 0.999854  , 0.99934024, ..., 0.98197997, 0.9433391 ,\n",
       "        0.6599177 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_survival_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc_266': 0.43548387096774194,\n",
       " 'auc_405': 0.30461922596754054,\n",
       " 'auc_782': 0.4206426484907498}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get only those RFS_all values for which relapse is 1\n",
    "\n",
    "{f\"auc_{t}\": auc[i] for i, t in enumerate(eval_times)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1362,  400,  323,  435,  196,  393, 2315,   88,  846,  526,  920,\n",
       "       4425,  410,  202,  592,  248,  330,   96])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([RFS_all[i] for i in range(len(RFS_all)) if relapse_all[i] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any elemt is greater than 1\n",
    "\n",
    "for i in range(len(pred_survival_all)):\n",
    "    if np.array(pred_survival_all)[i].max() > 1:\n",
    "        print(i, np.array(pred_survival_all)[i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_survival_all[15][0]\n",
    "\n",
    "# replace all the values greater than 1 with 1\n",
    "\n",
    "for i in range(len(pred_survival_all)):\n",
    "    for j in range(len(pred_survival_all[i])):\n",
    "        if pred_survival_all[i][j] > 1:\n",
    "            pred_survival_all[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_survival_all)[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_times = np.quantile(np.array([RFS_all[i] for i in range(len(RFS_all)) if relapse_all[i] == 1]), [.25, .5, .75]).astype(int)\n",
    "\n",
    "bs = brier_score_at_times(np.array(RFS_all), np.array(pred_survival_all), np.array(relapse_all), eval_times)\n",
    "auc = roc_auc_at_times(np.array(RFS_all), np.array(pred_survival_all), np.array(relapse_all), eval_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>bs_266</th>\n",
       "      <th>bs_405</th>\n",
       "      <th>bs_782</th>\n",
       "      <th>auc_266</th>\n",
       "      <th>auc_405</th>\n",
       "      <th>auc_782</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mtlr</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  bs_266  bs_405  bs_782  auc_266  auc_405  auc_782\n",
       "0  mtlr   0.051   0.092   0.139    0.405    0.305    0.421"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "metrics.append({\n",
    "    \"model\": \"mtlr\",\n",
    "    **{f\"bs_{t}\": bs[i] for i, t in enumerate(eval_times)},\n",
    "    **{f\"auc_{t}\": auc[i] for i, t in enumerate(eval_times)}\n",
    "})\n",
    "\n",
    "pd.DataFrame(metrics).round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
    "\n",
    "def compute_metric_at_times(metric, time_true, prob_pred, event_observed, score_times):\n",
    "    \"\"\"Helper function to evaluate a metric at given timepoints.\"\"\"\n",
    "    scores = []\n",
    "    for time, pred in zip(score_times, prob_pred.T):\n",
    "        target = time_true > time\n",
    "        uncensored = target | event_observed.astype(bool)\n",
    "        scores.append(metric(target[uncensored], pred[uncensored]))\n",
    "        \n",
    "    return scores\n",
    "\n",
    "\n",
    "def brier_score_at_times(time_true, prob_pred, event_observed, score_times):\n",
    "    scores = compute_metric_at_times(brier_score_loss, \n",
    "                                     time_true,\n",
    "                                     prob_pred,\n",
    "                                     event_observed,\n",
    "                                     score_times)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def roc_auc_at_times(time_true, prob_pred, event_observed, score_times):\n",
    "    scores = compute_metric_at_times(roc_auc_score, \n",
    "                                     time_true,\n",
    "                                     prob_pred, \n",
    "                                     event_observed,\n",
    "                                     score_times)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHecking weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3212284/3960777235.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ct_weight = torch.load('/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT-CLIP_v2.pt')\n",
      "/tmp/ipykernel_3212284/3960777235.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  my_w = torch.load('/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/model_weights_new/weight_020.pth')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ct_weight = torch.load('/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT-CLIP_v2.pt')\n",
    "my_w = torch.load('/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/model_weights_new/weight_020.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct_weight.keys()\n",
    "\n",
    "# rename the keys of the model weights. add \"clip.base_model.model.\" to all the keys\n",
    "\n",
    "ct_weight_new = {}\n",
    "for k in ct_weight.keys():\n",
    "    ct_weight_new[\"clip.base_model.model.\"+k] = ct_weight[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clip.base_model.model.temperature', 'clip.base_model.model.text_transformer.embeddings.position_ids', 'clip.base_model.model.text_transformer.embeddings.word_embeddings.weight', 'clip.base_model.model.text_transformer.embeddings.position_embeddings.weight', 'clip.base_model.model.text_transformer.embeddings.token_type_embeddings.weight', 'clip.base_model.model.text_transformer.embeddings.LayerNorm.weight', 'clip.base_model.model.text_transformer.embeddings.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.pooler.dense.weight', 'clip.base_model.model.text_transformer.pooler.dense.bias', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.0.0.weight', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.0.0.bias', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.1.0.weight', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.1.0.bias', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.2.weight', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.2.bias', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.1.weight', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.1.bias', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.2.weight', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.2.bias', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.3.weight', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.3.bias', 'clip.base_model.model.visual_transformer.to_patch_emb.1.weight', 'clip.base_model.model.visual_transformer.to_patch_emb.1.bias', 'clip.base_model.model.visual_transformer.to_patch_emb.2.weight', 'clip.base_model.model.visual_transformer.to_patch_emb.2.bias', 'clip.base_model.model.visual_transformer.to_patch_emb.3.weight', 'clip.base_model.model.visual_transformer.to_patch_emb.3.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.null_kv', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.q_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.k_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.3.0.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.3.0.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.3.1.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.3.4.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.null_kv', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.q_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.k_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.3.0.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.3.0.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.3.1.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.3.4.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.null_kv', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.q_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.k_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.3.0.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.3.0.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.3.1.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.3.4.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.null_kv', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.q_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.k_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.3.0.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.3.0.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.3.1.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.3.4.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.norm_out.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.norm_out.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.null_kv', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.q_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.k_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.3.0.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.3.0.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.3.1.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.3.4.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.null_kv', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.q_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.k_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.3.0.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.3.0.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.3.1.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.3.4.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.null_kv', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.q_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.k_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.3.0.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.3.0.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.3.1.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.3.4.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.null_kv', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.q_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.k_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.3.0.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.3.0.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.3.1.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.3.4.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.norm_out.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.norm_out.beta', 'clip.base_model.model.visual_transformer.vq._codebook.initted', 'clip.base_model.model.visual_transformer.vq._codebook.cluster_size', 'clip.base_model.model.visual_transformer.vq._codebook.embed', 'clip.base_model.model.visual_transformer.to_pixels_first_frame.0.weight', 'clip.base_model.model.visual_transformer.to_pixels_first_frame.0.bias', 'clip.base_model.model.visual_transformer.to_pixels.0.weight', 'clip.base_model.model.visual_transformer.to_pixels.0.bias', 'clip.base_model.model.to_text_latent.weight', 'clip.base_model.model.to_visual_latent.weight', 'clip.base_model.model.to_text_latent_extra.weight', 'clip.base_model.model.to_visual_latent_extra.weight'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_weight_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.base_layer.weight'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(my_w.keys())[228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.weight'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ct_weight_new.keys())[228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.vq._codebook.cluster_size 352\n",
      "clip.base_model.model.visual_transformer.vq._codebook.embed 353\n"
     ]
    }
   ],
   "source": [
    "for i, k in enumerate(ct_weight_new.keys()):\n",
    "    try:\n",
    "        if not torch.equal(ct_weight_new[k], my_w[k]):\n",
    "            print(k, i)\n",
    "    except:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
